\section{Platform}

\subsection{overview: short summary of what the platform does}

\subsection{Overall design principles}

In designing the platform, we tried to achieve a good combination of the design principles described below:

\textbf{Support to human-machine workflows} The need to rely on hybrid human-machine workflows was a key assumption in our research. The solution was required to automate as much as possible of the data creation / curation process while effectively integrating human components, from the crowdsourcing of part of the data to the administration of the process itself.

\textbf{Data re-use} Making the best possible use of pre-existing, reliable data was a key design driver, in particular to take advantage of the substantial volume of open data published in the UK over the last two-three years, and expecting a similar trend to be observable in many other countries in the upcoming years.

\textbf{Open source software} It was assumed that any software component required by the solution could be built using open source software only, also to preserve any available budget for the compensation of paid contributors to the crowdsourcing campaigns.

\textbf{Scalability and high availability} The solution was designed to be highly scalable and available, beyond the needs of the experiments described later in this document and suitable for real-world deployment.

\textbf{Versatility} The solution needed being versatile and suitable to support different workflows, input data sources and target output datasets. Despite the extensive literature, crowdsourcing in particular is still more of a craft than science and no design formula can assure success without experimentation. The design of the solution needed to support different forms of crowdsourcing across its many dimensions: paid contributors vs volunteers, results aggregation, quality assessment etc.

\subsection{Crowdsourcing component design principles}

The following is a high level description of the crowdsourcing component of the platform according to the dimensions presented in \cite{Wearethedata:2015uo}. The description is independent of the specific data creation / curation objective the system could be deployed to address. 

A general observation is that the crowdsourcing element was intentionally designed to explore characteristics that are far from what has been already extensively explored in VGI literature, particularly thanks to the opportunity to study the OpenStreetMap case. The hypothesis and ambition is that the finding of the research can be complementary to what can be achieved with VGI and enrich the set of tools available to the system designer.

\textbf{What is outsourced} The objective of the outsourcing activity is the production of original geospatial data or the correction / validation of pre-existing data, wherever the activity can be only performed by human agents. 

We use the term "geospatial data" in its wider connotation: data that relates to or is associated with a particular location. It can be about the geographic characteristics of a place (e.g. the topology of a street network), but also about facts associated to that place (e.g. how many trees in that street).

The activity typically translates into surveying the locations or examining imagery thereof and record observations (e.g. "how many trees can be seen from longitude x and latitude y?"), or, alternatively, amend some previous recording of the same (e.g. "can you confirm that there is a hospital in Vicarage Rd, Watford, Hertfordshire?"). 

To avoid the physical survey of the locations, participants examine publicly available imagery of the location. This is not uncommon, e.g. the OpenStreetMap website uses aerial imagery sourced from Microsoft Bing to let its contributors edit the topology of the streets\footnote{See \url{https://www.openstreetmap.org/edit}.}. The cost effectiveness of humans observers is expected to outperform machine learning, computer vision and automation in general for a few more years, particularly in observing imagery that are ambiguous [IT WOULD BE NICE TO QUOTE SOMEONE HERE].

\textbf{Who is the crowd} No specialised knowledge or skills are required, nor a connection to the places being surveyed. In VGI, though, contributors are often associated to the locations they work on, as it is both a driver for their motivation and direct knowledge of the place is occasionally necessary to assure the completeness of the data. The function of buildings or the location of mailboxes, for example, can't be inferred from observing OpenStreetMap's aerial imagery. Moreover, the first survey of new locations, e.g. new developments before they are visible on aerial imagery, are often made using specialised equipment, e.g. to record GPS tracks\footnote{E.g. the OpenStreetMap project advises against using conventional mobile phone GPS functionality, as it can be very difficult to work out the technical accuracy of a track. See \url{http://wiki.openstreetmap.org/wiki/Recording_GPS_tracks}.}.

The crowdsourcing task design intends to assure the contributors detachment from the locations.

\textbf{How are the task outsourced} Again, to explore different formulae than what is common in VGI, the crowdsourcing component focus is on implementing micro rather than macro tasks, and, definitely, microtasks cannot be burdened with the overhead typical of surveying in the real world, like planning a journey and travelling to the location and back, use equipment etc. 

Wherever possible, one's contribution is limited to max few minutes of work at the computer, the shorter the time required the better. Ideal tasks are closed- rather than open-ended and require no particular thought or focus. The participant has no visibility of the workflow she is part of: the volume of effort spent in preparing the data, aggregating the responses, assessing their accuracy etc. 

\textbf{Why do people contribute} In VGI, crowds are made of intrinsically motivated volunteers: open data advocates, geospatial practitioners and individuals who are passionate about geography in general [SOME REFERENCE]. Our contributors are oblivious of the the context of the project, have no personal connection to the locations being surveyed, and likely are unaware of and find no motivation in contributing to the "cause" of open data in general. They perform their tasks driven merely by the financial reward. This is typically the crowd that can be recruited through mainstream crowdwork platforms such as Amazon Mechanical Turk and Crowdflower. 

\subsection{Implementation}

\subsubsection{Components} \leavevmode \\ %% Why is this necessary to get a new line?

\textbf{CrowdFlower} CrowdFlower is a crowdsourcing Software-as-a-Service platform specialised in hosting data-centred microtasks for volunteer or paid participants. Clients who accept for their data to be re-published as open data by CrowdFlower may access the service under the "Data for Everyone" plan, that has no costs but for the compensation to the paid participants and a 20\% commission. The service is available both through a templating system called "CrowdFlower Markup Language" (or "CML") that can be customised by using CrowdFlower's web-based tools, or through APIs. 

When using CML, the options available to implement a crowdsourcing model - such as how Worker accuracy is assessed or how results are aggregated - are limited by the functionality supported by the system\footnote{For example, CrowdFlower calculates the Workers' agreement simply as the \% of participants who expressed the majority vote, and tasks can be stopped automatically when a target agreement \% is achieved. If one wanted to use a stronger statistical tool to calculate agreement, such as Fleiss' Kappa, that automation cannot be used and the calculation must take place outside of the system.}. In order not to introduce additional components in the system and maximise scalability and high availability, it was decided to rely on CML only and work around some of its limitations by performing non-performance critical tasks offline, e.g. results aggregation. 

\textbf{Google Maps} Google Maps is a Software-as-a-Service mapping service for the web and mobile devices made available by Google to the public for free within the limitations of some terms of service. It offers satellite and aerial imagery, maps, interactive panoramic views of streets ("Street View") and the respective metadata through APIs. Many of the services can be embedded in third party websites and customised using client-side JavaScript, hence making it very suitable to be integrated with CrowdFlower or any other template-based content management system. 

The prototype uses Google's services as the team had already experience of programming their APIs, but other equivalent services could have been used, e.g. Microsoft's Bing Maps. OpenStreetMap's aerial imagery for example is currently based on the latter\footnote{See \url{http://wiki.openstreetmap.org/wiki/Bing#Bing_Aerial_Imagery}, last accessed 2 January 2016.}.

\textbf{PostgreSQL + PostGIS} All data from primary sources was imported into a PostgreSQL relational database\footnote{See \url{http://www.postgresql.org/}.} running the PostGIS spatial extender \footnote{See \url{http://postgis.net/}.}. All geospatial data was converted from its original format (CSV, ESRI Shapefile etc.) to PostGIS' native geospatial types to be consistent across all sources and enabling spatial and geographic querying. When needed, QGIS \footnote{See \url{http://www.qgis.org/}.} was used to visually inspect the data on maps.

\textbf{Scripting} Bash, NodeJS and PostgreSQL SQL scripting were used to glue all component together wherever automation was possible across the many stages of: (i) ingestion and preparation of the reference data\footnote{See the GitHub repository at \url{https://github.com/Digital-Contraptions-Imaginarium/OLAF-yr2_reference_data}.}, (ii) inference of house numbers where made possible from LRPP data\footnote{See the GitHub repository at \url{https://github.com/Digital-Contraptions-Imaginarium/OLAF-yr2_inference_data}.} and (iii) creation of the data for the crowdsourcing component and elaborating its results\footnote{See the GitHub repository at \url{https://github.com/Digital-Contraptions-Imaginarium/OLAF-yr2_lab}, {\it data-prep-scripts} and {\it analysis-scripts} folders respectively.}.  

\subsubsection{Architecture}

[SOME NICE DIAGRAM]