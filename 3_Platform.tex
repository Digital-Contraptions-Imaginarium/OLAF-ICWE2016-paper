\section{Platform}

\subsection{overview: short summary of what the platform does}

\subsection{Core design principles}

In designing the platform, we tried to achieve a good combination of the design principles described below:

\textbf{Support to human-machine workflows} The need to rely on hybrid human-machine workflows was a key assumption in our research. The solution was required to automate as much as possible of the data creation / curation process while effectively integrating human components, from the crowdsourcing of part of the data to the administration of the process itself.

\textbf{Data re-use} Making the best possible use of pre-existing, reliable data was a key design driver, in particular to take advantage of the substantial volume of open data published in the UK over the last two-three years, and expecting a similar trend to be observable in many other countries in the upcoming years.

\textbf{Open source software} It was assumed that any software component required by the solution could be built using open source software only, also to preserve any available budget for the compensation of paid contributors to the crowdsourcing campaigns.

\textbf{Scalability and high availability} The solution was designed to be highly scalable and available, beyond the needs of the experiments described later in this document and suitable for real-world deployment.

\textbf{Versatility} The solution needed being versatile and suitable to support different workflows, input data sources and target output datasets. Despite the extensive literature, crowdsourcing in particular is still more of a craft than science and no design formula can assure success without experimentation. The design of the solution needed to support different forms of crowdsourcing across its many dimensions: paid contributors vs volunteers, results aggregation, quality assessment etc.

\subsection{Crowdsourcing component... WHAT?}

[ELENA: introduce the platform according to the dimensions I use in my presentations. Note that this is a description of the platform as we envision it for the future, not as bespoke tool to elicit house numbers]

The following is a high level description of the crowdsourcing component of the platform according to the dimensions presented in \cite{Wearethedata:2015uo}. The description is independent of the specific objective the system could be deployed to address.

\textbf{What is outsourced} Because of the core design principles above, the scope of the outsourcing activity is centred around the production of new geospatial data or the correction / validation of pre-existing data, wherever the activity could not be performed but by engaging human agents. 

This typically translates into surveying the locations to observe some phenomenon and record the observation (e.g. "how many trees can be seen from longitude x and latitude y?"), or, alternatively, amend some previous recording of the same (e.g. "can you confirm that there is a hospital in Vicarage Rd, Watford, Hertfordshire?"). 

To fully leverage the crowd and in an attempt to maximise cost effectiveness, publicly available imagery of the locations is used rather than surveying the actual places. The performance of humans observers is expected to keep outperforming machine learning, computer vision and automation in general for a few more years, particularly in observing imagery that are ambiguous [IT WOULD BE NICE TO QUOTE SOMEONE HERE].

\textbf{Who is the crowd} VGI literature explored extensively how geospatial data can be created by crowds of intrinsically motivated volunteers, often taking responsibility of tasks that require substantial effort, such as surveying a street in the real world, including travel to get to the location, the use of equipment to record GPS tracks etc. 

Our platform was intentionally designed to explore the somehow opposite scenario. Our contributors are oblivious of the the context of the project, have no personal connection to the locations being surveyed, and likely find no motivation in contributing to the "cause" of open data in general. They perform survey {\it micro}tasks and are driven merely by the financial reward. This is typically the crowd we can recruit through the mainstream paid crowdsourcing platforms such as Amazon Mechanical Turk and Crowdflower. 

\textbf{How are the task oursourced}

\textbf{Why do people contribute}


what/who/how

what to crowdsource (inputs/outputs/creation vs validation tasks)
who is the crowd 
how: task breakdown, open vs closed answers, incentives etc etc etc

\subsection{implementation}