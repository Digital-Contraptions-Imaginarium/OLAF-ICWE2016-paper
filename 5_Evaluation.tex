\section{Evaluation}

\subsection{Data}

WODA was deployed for a specific geographic sample, that is the same five OS 1:10,000 raster tiles that were previously used in literature to analyse the performance of GI crowdsourcing and originally selected by Haklay in \cite{Haklay:2010vs} to assess OSM\footnote{The tiles are: TQ37ne, TQ28ne, TQ29nw, TQ26se and TQ36nw.}. This is an area of ~113 $ km^2 $ of Greater London that includes 3,982 named roads. 

\subsection{Evaluation metrics}

The evaluation is aimed at observing WODA's effectiveness at producing house numbers for the sample. Given the premises described in chapter \ref{introduction}, a financial metric is used to measure performance, that is the average cost per road of using crowdsourcing for data production. 

\subsection{Results}

\textbf{Ingestion of primary data sources} The data obtained by implementing the approach described in section \ref{crowdsourcing-olaf} successfully populated house numbers from LRPP for 82\% of the streets in OSON.

\textbf{House number inference} The conditions necessary to apply the inference algorithms, before using any crowdsourcing, were verified for 74\% of roads. Applied to these, algorithms \ref{algo:inference-numbers} and \ref{algo:inference-numbers-suffix} generated ~113k house numbers in addition to the already known ~111k (+102\%). 

\textbf{House number crowdsourcing} Stop condition {\it s.2} - with 118 repeat judgments vs 117 first judgements - was verified at the end of three crowdsourcing rounds.  The data for only 4 roads only was collected successfully, with an average cost of 9.00 USD per road. 

To better examine any trends in Worker behaviour and consensus through iterations, three additional rounds were run, too. 

More than 12\% of Workers were caught submitting judgements earlier than the 90 seconds limit, and were excluded from contributing further. 19\% failed the test of copying the name of the road in the form at least once, and thus were identified as not credible and their judgements ignored. The average time it took for credible Workers to complete the tasks was 6:14. 83\% of them were always faster than 3:00, that means that they never watched the instructions video in full before submitting.

The extra three rounds showed how agreement on most roads not only failed to converge, but stalled or worsened with new iterations.\footnote{See \url{https://github.com/Digital-Contraptions-Imaginarium/OLAF-yr2_lab/tree/gh-pages/docs#fleiss-kappa-vs-iterations}.} 

