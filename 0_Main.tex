\documentclass{llncs}

\usepackage[ruled]{algorithm2e}
\usepackage{floatrow}
\usepackage[utf8]{inputenc}

\usepackage{url}
%% This was made necessary to break` long URLs, no other system I tried worked and a few URLs still fail; see http://tex.stackexchange.com/a/10419/60582 
\makeatletter
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\makeatother

%% This is to be able to use " to do proper double quotes instead of ''
\usepackage [english]{babel}
\usepackage [autostyle, english = american]{csquotes}
\MakeOuterQuote{"}

\usepackage{graphicx}
\graphicspath{{./images/}}

\title{Hybrid human-machine workflows to create and curate geospatial data}

\author{AUTHOR 1\inst{1}, AUTHOR 2\inst{1} \and AUTHOR 3\inst{2}}
\institute{INSTITUTE 1 \email{EMAIL FOR AUTHOR 1} \and INSTITUTE 2}

\date{December 2015}

\begin{document}

\maketitle

\begin{abstract}
As more open data is published by governments and organisations, the task of creating new or improving existing data evolves from being a ground-up process - where data is collected and shaped from scratch - to an enhancement process, where pre-existing sources and original additions merge into new, valuable data products. 

Machine learning, computer vision and automation in general can't (yet?) address all challenges in this area. Available data, computation and original contributions by human participants - e.g. through crowdsourcing - need integration and enabling each other through hybrid human-machine workflows. 

In this paper we present an experimental platform that implements this model, aimed specifically at the creation and curation of geospatial data. The platform is then applied to a real world use case: the creation of "OLAF", an open dataset of all valid UK addresses, starting from available open data, augmenting it through computational inference and complementing it by using microtask crowdsourcing. Experimental evaluation shows the feasibility, effectiveness and challenges of the approach.
\end{abstract}

\begin{keywords}
Crowdsourcing, Geographic Information, hybrid human-machine systems, open data 
\end{keywords}

[REVIEW NOTES IN THIS DOCUMENT ARE SHOWN IN CAPITALISED LETTERS AND / ON IN SQUARE BRACKETS] 
\begin{itemize}

    \item MAX 18 PAGES
    \item OPEN POINTS
        \begin{itemize}
            \item TOO LONG
            \item NO REFERENCES TO ANY HUMAN-MACHINE WORKFLOW LITERATURE
            \item MANY READERS DID NOT GET THE CONCEPT OF DATA "CURATION" AS A GENERIC TERM TO INCLUDE ALL ACTIVITIES RELATED TO THE DATA MAINTENANCE, FIXING ERRORS, UPDATING ETC.  PERHAPS IT'S NECESSARY TO MOVE TO ANOTHER TERM
            \item WAS IT ELENA TO SUGGEST NOT TO TALK ABOUT OPEN DATA IN THE TITLE AND ABSTRACT? I BELIEVE IT COULD GIVE ME A LITTLE EDGE INSTEAD
            \item AFTER THE FOCUS WAS SHIFTED TOWARDS PRESENTING THE "PLATFORM" IT IS USEFUL TO HIGHLIGHT THIS CAN BE EASILY USED TO CURATE DATA (E.G. FIX ERRROS) NOT ONLY CREATE IT, AS THE ORIGINAL TITLE SAID. I'VE DONE SOME OF THESE ADJUSTMENTS ALREADY
            \item ALL THE REFERENCE METADATA NEEDS BEING CHECKED, IN PARTICULAR FOR THOSE FIELDS THAT THE REFERENCE MANAGER MAY HAVE GUESSED WRONG
            \item IF THE REVIEW IS BLIND, WILL THE READMES IN THE GITHUB REPOSITORY REVEAL WHO WE ARE? SHOULD THEY BE HIDDEN?
        \end{itemize}
\end{itemize}

\input{1_Introduction.tex}
\input{2_Background_and_related_work.tex}
\input{3_Platform.tex}
\input{4_Crowdsourcing_OLAF.tex}
\input{5_Evaluation.tex}
\input{6_Discussion.tex}
\input{7_Conclusions.tex}

\textbf{Acknowledgements.} [REMOVED IF REVIEW IS BLIND]

\input{compile_bibitems.bbl}

\end{document}

