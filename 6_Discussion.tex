\section{Discussion}

From a merely technical perspective, the experience of implementing WODA confirmed that it is feasible to implement a system that supports non-trivial human-machine hybrid workflows, while relying only on the scalability and availability of third party services only.

Conversely, from a functional point of view, many of CrowdFlower's characteristics ended up not to be compatible with the needs of our design. So, while we achieved avoiding custom Worker-facing software components by relying exclusively on CrowdFlower's native features, we were also forced to implement many ad hoc workarounds. Several of these solutions were far from ideal and affected negatively the performance of the overall system to a point compromising its effectiveness and nullifying the advantages. 

For example, not having the possibility to rely on CrowdFlower's own metric (quorum vs Fleiss' kappa) did not allow us to calculate consensus as new judgements came into the system, but only offline, between crowdsourcing rounds. This forced us to collecting more judgements than necessary. In turn, we also lost the option to use CrowdFlower's features that prevent Workers to judge the same item more than once, causing an overwhelming volume of unwanted, repeated judgements. 

An equally pressing issue is the choice of crowdsourcing task. The complexity and an open-ended nature of the surveying task failed to catalyse the contributors' agreement and produce results that are statistically credible. This suggests a substantial doubt on the effectiveness of crowdsourcing survey activities of this sort. Gottlieb {\it et al.} in \cite{Gottlieb:2012fh} faced similar difficulty, e.g. in exploring crowdsourcing to geolocate places in videos.

Finally, any discussion should still be filtered through a cost / benefit examination: what is a "reasonable cost" of producing one address with a target degree of confidence. The volume of roads we could enable inference for was too little to make such considerations.
