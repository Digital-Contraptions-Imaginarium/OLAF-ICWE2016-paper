\section{Discussion}

From a merely technical perspective, the experience of implementing WODA confirmed that it is feasible to implement a system that supports non-trivial human-machine hybrid workflows while relying only on the scalability and availability of third party services only.

Conversely, from a functional point of view, many of the chosen services' characteristics - and of the crowdsourcing platform in particular - ended up not to be compatible with the needs of our design. So, while we achieved avoiding custom Worker-facing software components by relying exclusively on CrowdFlower and Google Map's native features, we were also forced to implement many ad hoc workarounds. Several of these arrangements were far from ideal and affected negatively the performance of the overall system, to a point compromising its effectiveness and advantages. 

For example, not having the possibility to rely on CrowdFlower's own metric (quorum vs Fleiss' kappa) did not allow us to calculate consensus as new judgements came into the system, but only offline, between crowdsourcing rounds. This forced us to collect more judgements than necessary. In turn, we also lost the option to use CrowdFlower's features that prevent Workers to judge the same item repeatedly, causing an overwhelming volume of unwanted judgements that stopped the experiment very early in respect to our expectations. 

An equally pressing issue is the choice of crowdsourcing task, that failed to catalyse the contributors' agreement and produce results that are statistically credible. This suggests a substantial doubt on the effectiveness of crowdsourcing survey activities of this sort. We reckon that the key cause was a combination of (i) conventional cheating, (ii) the complexity and open-ended nature of the surveying task and (iii) some degree of sloppiness of participants even when in good faith. When aiming at such a high consensus target, as in our case, even one single point of disagreement can substantially negatively impact the metric.

Cheating in crowdsourcing does not require explanation and is common to most systems. The way the task was designed, it was easy for participants to just state that the surveyed street offered no house numbers. 

The act of surveying a location - in person as in the interactive imagery - is not trivial. Some of its dynamics may seem intuitive, however not all participants would necessarily grasp them, even after watching the instructions video. Gottlieb {\it et al.} in \cite{Gottlieb:2012fh} faced similar difficulty, e.g. in exploring crowdsourcing to geolocate places in videos.

In terms of sloppiness of the surveys, even in good faith many Workers may have been content when finding house numbers that looked "small enough" or "high enough", and interrupted their search there, despite what was stated in the instructions. 

Finally, any discussion should still be filtered through a cost / benefit examination: what is a "reasonable cost" of producing one address with a target degree of confidence. The volume of roads we could enable inference for was too little to make such considerations.
